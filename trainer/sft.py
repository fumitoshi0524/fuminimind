import os
import sys

__package__ = "trainer"
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

import argparse
import time
import warnings
import torch
import torch.distributed as dist
from contextlib import nullcontext
from torch import optim, nn
from torch.nn.parallel import DistributedDataParallel
from torch.utils.data import DataLoader, DistributedSampler

from model.model import fuminimindConfig
from dataset.lm_dataset import SFTDataset
from trainer.train_utils import (
    get_lr,
    Logger,
    is_main_process,
    lm_checkpoint,
    init_distributed_mode,
    setup_seed,
    init_model,
    SkipBatchSampler,
)

warnings.filterwarnings("ignore")


def train_epoch(epoch, loader, iters, start_step=0, wandb=None):
    loss_fct = nn.CrossEntropyLoss(ignore_index=-100, reduction="mean")
    start_time = time.time()

    for step, (input_ids, labels) in enumerate(loader, start=start_step + 1):
        input_ids = input_ids.to(args.device)
        labels = labels.to(args.device)

        lr = get_lr(epoch * iters + step, args.epochs * iters, args.learning_rate)
        for param_group in optimizer.param_groups:
            param_group["lr"] = lr

        shift_labels = labels[..., 1:].contiguous()
        valid_count = (shift_labels != -100).sum().item()
        if valid_count == 0:
            Logger(f"SFT invalid labels at step {step}: all shift labels masked")

        if hasattr(lm_config, "vocab_size"):
            max_id = int(input_ids.max().item())
            min_id = int(input_ids.min().item())
            if min_id < 0 or max_id >= lm_config.vocab_size:
                Logger(
                    f"SFT input_ids out of range at step {step}: min={min_id}, max={max_id}, vocab_size={lm_config.vocab_size}"
                )
                raise RuntimeError("SFT input_ids out of range")

        with autocast_ctx:
            res = model(input_ids)
            logits = res.logits
            if torch.isnan(logits).any() or torch.isinf(logits).any():
                Logger(f"SFT logits NaN/Inf at step {step}")
                raise RuntimeError("SFT logits NaN/Inf")
            shift_logits = logits[..., :-1, :].contiguous()
            loss = loss_fct(
                shift_logits.view(-1, shift_logits.size(-1)),
                shift_labels.view(-1),
            )
            loss = loss / args.accumulation_steps

        if not torch.isfinite(loss):
            Logger(
                f"SFT loss NaN/Inf at step {step}: valid_count={valid_count}, logits_min={logits.min().item():.6f}, logits_max={logits.max().item():.6f}"
            )
            raise RuntimeError("SFT loss NaN/Inf")

        scaler.scale(loss).backward()

        if (step + 1) % args.accumulation_steps == 0:
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(model.parameters(), args.grad_clip)
            scaler.step(optimizer)
            scaler.update()
            optimizer.zero_grad(set_to_none=True)

        if step % args.log_interval == 0 or step == iters - 1:
            spend_time = time.time() - start_time
            current_loss = loss.item() * args.accumulation_steps
            current_lr = optimizer.param_groups[-1]["lr"]
            eta_min = spend_time / (step + 1) * iters // 60 - spend_time // 60
            Logger(
                f"Epoch:[{epoch + 1}/{args.epochs}]({step}/{iters}) loss:{current_loss:.6f} lr:{current_lr:.12f} epoch_Time:{eta_min}min:"
            )
            if wandb:
                wandb.log({"loss": current_loss, "lr": current_lr, "epoch_Time": eta_min})

        if (step % args.save_interval == 0 or step == iters - 1) and is_main_process():
            model.eval()
            moe_suffix = (
                "_moe" if hasattr(lm_config, "use_moe") and lm_config.use_moe else ""
            )
            ckp = f"{args.save_dir}/{args.save_weight}_{lm_config.hidden_size}{moe_suffix}.pth"

            if isinstance(model, torch.nn.parallel.DistributedDataParallel):
                state_dict = model.module.state_dict()
            else:
                state_dict = model.state_dict()

            state_dict = {k: v.half() for k, v in state_dict.items()}
            torch.save(state_dict, ckp)

            lm_checkpoint(
                lm_config,
                weight=args.save_weight,
                model=model,
                optimizer=optimizer,
                scaler=scaler,
                epoch=epoch,
                step=step,
                wandb=wandb,
                save_dir="checkpoints",
            )

            model.train()


def get_parser():
    parser = argparse.ArgumentParser(description="fuminimind Full SFT")
    parser.add_argument("--save_dir", type=str, default="out", help="path to save the model")
    parser.add_argument("--save_weight", default="full_sft", type=str, help="prefix for saving weights")
    parser.add_argument("--epochs", type=int, default=2, help="number of training epochs")
    parser.add_argument("--batch_size", type=int, default=16, help="batch size")
    parser.add_argument("--learning_rate", type=float, default=1e-6, help="initial learning rate")
    parser.add_argument(
        "--device",
        type=str,
        default="cuda:0" if torch.cuda.is_available() else "cpu",
        help="training device",
    )
    parser.add_argument("--dtype", type=str, default="bfloat16", help="mixed precision type")
    parser.add_argument("--num_workers", type=int, default=8, help="number of data loading workers")
    parser.add_argument("--accumulation_steps", type=int, default=1, help="gradient accumulation steps")
    parser.add_argument("--grad_clip", type=float, default=1.0, help="gradient clipping threshold")
    parser.add_argument("--log_interval", type=int, default=100, help="log printing interval")
    parser.add_argument("--save_interval", type=int, default=1000, help="model saving interval")
    parser.add_argument("--hidden_size", default=512, type=int, help="hidden layer dimension")
    parser.add_argument("--num_hidden_layers", default=8, type=int, help="number of hidden layers")
    parser.add_argument("--max_seq_len", default=512, type=int, help="maximum sequence length for training")
    parser.add_argument(
        "--use_moe",
        default=0,
        type=int,
        choices=[0, 1],
        help="whether to use MoE architecture (0=no, 1=yes)",
    )
    parser.add_argument(
        "--data_path",
        type=str,
        default="dataset/sft.jsonl",
        help="sft data path",
    )
    parser.add_argument(
        "--from_weight",
        default="pretrain",
        type=str,
        help="which weight to train from, 'none' means training from scratch",
    )
    parser.add_argument(
        "--from_resume",
        default=0,
        type=int,
        choices=[0, 1],
        help="whether to automatically detect & resume training (0=no, 1=yes)",
    )
    parser.add_argument("--use_wandb", action="store_true", help="whether to use wandb")
    parser.add_argument(
        "--wandb_project", type=str, default="fuminimind-Full-SFT", help="wandb project name"
    )
    return parser


def run(parsed_args):
    global autocast_ctx, model, optimizer, scaler, lm_config, args
    args = parsed_args
    local_rank = init_distributed_mode()
    if dist.is_initialized():
        args.device = f"cuda:{local_rank}"

    setup_seed(42 + (dist.get_rank() if dist.is_initialized() else 0))
    os.makedirs(args.save_dir, exist_ok=True)

    lm_config = fuminimindConfig(
        hidden_size=args.hidden_size,
        num_hidden_layers=args.num_hidden_layers,
        use_moe=bool(args.use_moe),
    )

    ckp_data = (
        lm_checkpoint(lm_config, weight=args.save_weight, save_dir="checkpoints")
        if args.from_resume == 1
        else None
    )

    device_type = "cuda" if "cuda" in args.device else "cpu"
    dtype = torch.bfloat16 if args.dtype == "bfloat16" else torch.float16
    autocast_ctx = (
        nullcontext() if device_type == "cpu" else torch.cuda.amp.autocast(dtype=dtype)
    )

    wandb = None
    if args.use_wandb and is_main_process():
        import swanlab as wandb

        wandb_id = ckp_data.get("wandb_id") if ckp_data else None
        resume = "must" if wandb_id else None
        wandb_run_name = (
            f"fuminimind-Full-SFT-Epoch-{args.epochs}-BatchSize-{args.batch_size}-LearningRate-{args.learning_rate}"
        )
        wandb.init(project=args.wandb_project, name=wandb_run_name, id=wandb_id, resume=resume)

    model, tokenizer = init_model(lm_config, args.from_weight, device=args.device)
    train_ds = SFTDataset(args.data_path, tokenizer, max_length=args.max_seq_len)
    train_sampler = DistributedSampler(train_ds) if dist.is_initialized() else None
    scaler = torch.cuda.amp.GradScaler(enabled=(args.dtype == "float16"))
    optimizer = optim.AdamW(model.parameters(), lr=args.learning_rate)

    start_epoch, start_step = 0, 0
    if ckp_data:
        model.load_state_dict(ckp_data["model"])
        optimizer.load_state_dict(ckp_data["optimizer"])
        scaler.load_state_dict(ckp_data["scaler"])
        start_epoch = ckp_data["epoch"]
        start_step = ckp_data.get("step", 0)

    if dist.is_initialized():
        model._ddp_params_and_buffers_to_ignore = {"freqs_cos", "freqs_sin"}
        model = DistributedDataParallel(model, device_ids=[local_rank])

    for epoch in range(start_epoch, args.epochs):
        if train_sampler:
            train_sampler.set_epoch(epoch)

        if epoch == start_epoch and start_step > 0:
            batch_sampler = SkipBatchSampler(
                train_sampler or range(len(train_ds)), args.batch_size, start_step + 1
            )
            loader = DataLoader(
                train_ds,
                batch_sampler=batch_sampler,
                num_workers=args.num_workers,
                pin_memory=True,
            )
            Logger(
                f"Epoch [{epoch + 1}/{args.epochs}]: skip first {start_step} steps, starting from step {start_step + 1}"
            )
            train_epoch(epoch, loader, len(loader) + start_step + 1, start_step, wandb)
        else:
            loader = DataLoader(
                train_ds,
                batch_size=args.batch_size,
                shuffle=(train_sampler is None),
                sampler=train_sampler,
                num_workers=args.num_workers,
                pin_memory=True,
            )
            train_epoch(epoch, loader, len(loader), 0, wandb)


def main():
    args = get_parser().parse_args()
    run(args)


if __name__ == "__main__":
    main()
